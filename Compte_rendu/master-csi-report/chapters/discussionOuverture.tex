\chapter*{Discussion et Ouverture}

\section*{Discussion}


Les résulats obtenus grâce à l'analyse d'Érysichthon ne permettent pas encore de conclure sur la sécurité globale de la bibliothèque Hacl*, il reste encore trop de fonctions non analysées ($\sim 39\%$). En revanche les premiers résultats sont encourageants et montrent que l'utilisation des contre-mesures développées au chapitre \ref{chap:constantTimeSolution} est effectivement une bonne pratique. Compléter Érysichthon pour avoir une analyse complète est en tête de liste de la liste des tâches du projet.\medbreak

Actuellement, un seul compilateur a permis de produire ces résultats, il faut absolument étendre l'utilisation à d'autres compilateurs pour pouvoir croiser les résultats et avoir une étude plus complète quant à la sécurité de cette bibliothèque.\smallbreak

\subsection*{Retour sur les résultats}

Nous savons que les optimisations du compilateur modifient le binaire et peuvent insérer des fuites parce que ces modifications changent la structure du programme. Avec cet outil, au lieu d'appeler frontalement \texttt{-O2}, nous pouvons plutôt appeler nominativement les options qui se cachent derrière : \texttt{-falign-functions}, \texttt{-falign-jumps}, \etc. Cette solution permettrait d'identifier les passes de compilateur qui sont déterminantes pour l'apparition de failles. Il faut cependant être attentif avec cette solution car \texttt{GCC}, utilisé ici, fonctionne différemment de \texttt{LLVM+Clang}. Ce dernier emploie une représentation interne pour effectuer la transformation entre un programme source et un programme binaire. Cette représentation interne évolue entre les différentes étapes de compilation. Cela induit que certaines étapes, même si elles ne nous plaisent pas car elles ajoutent des failles, sont nécessaires pour les suivantes. Nous avons notamment pu croiser ce cas de figure durant notre période de test et, à moins d'avoir un code adapté à chaque compilateur, la solution retenue a été de désactiver les optimisations du compilateur pour le bloc de code concerné.\medbreak

En poursuivant dans cette voie, il nous est aussi possible d'identifier précisément les fonctions qui ont besoin d'être sécurisées et d'adapter leur compilation. Nous pourrions avoir une liste \texttt{sécurisées} qui identifie les fonctions dont la compilation induit trop de dégâts et les autres seraient compilées avec un niveau d'optimisation plus élevé. Par exemple, si \textit{A.c} est compilé avec \texttt{-O0}, \textit{A.o} sera généré sans optimisation. De même, si \textit{B.c} est compilé avec \texttt{-O3}, \textit{B.o} contiendra des fonctions optimisées. Ainsi, dans le binaire final \textit{C.o}, les appels vers une fonction de \textit{A.o} sont des instructions de saut vers le code compilé avec \texttt{-O0}, et les appels vers \textit{B.o} des sauts vers du code compilé avec \texttt{-O3}. Nous obtenons un mélange de fonctions optimisées et non optimisées dans le même exécutable. Cette solution réduira les performances globales mais préservera un niveau de sécurité plus élevé. Cette solution ouvre la voie à un nouveau type d'étude possible : étude des effets d'utilisation de différents niveaux d'optimisation au sein d'un même projet.\medbreak

\subsection*{Implémentation dans le cadre d'une CI}

Utiliser Érysichthon c'est retrouver des vulnérabilités documentées, il nous a aussi permis d'identifier une vingtaine de fonctions présentant des fuites (même si la sécurité n'est pas engagée). Donc ajouter Érysichthon à une chaîne de tests, \textit{CI} ou Intégration Continue en langue de Molière, est l'objectif principal que nous nous fixons. \citeauthor{schneider2024breakingbadcompilersbreak}, notre référence, énumérait une liste de solutions permettant de garantir la sécurité temps constant d'une bibliothèque. Les solutions notables étaient de mettre en place un "\textit{distributeur}" de binaire, permettant au développeur de piocher selon ses besoins avec bien entendu tous les codes sources publiques, ou d'effectuer massivement des tests. La voie prise pour assurer la sécurité d'HACL* s'aligne avec cette dernière solution. Et ajouter cet outil à un mécanisme d'intégration continue permettra d'avoir des rapports réguliers dès la publication d'une nouvelle version de HACL* (lors d'ajouts de nouvelles primitives cryptographiques ou de modifications du code source).\smallbreak

L'avantage indéniable de cette méthode est que les failles seront découvertes au fur et à mesure des tests effectués sur des compilateurs de plus en plus récents, limitant le risque d'avoir une faille se révélant dans la nature.


\section*{Ouverture}

La sécurité des binaires face aux attaques temporelles n'est pas chose aisée. Nous nous sommes concentrés avec ce projet sur la vérification de binaires et la conception d'une certification de sécurité vers un compilateur. Nous considérons le compilateur comme une boîte noire. Cette approche se justifie par la non mainmise sur le développement de tels projets. Historiquement les ordinateurs étaient lents et la conception du logiciel devait être réalisée avec précision pour optimiser ou guider la conception des binaires. Les compilateurs et leurs optimisations ont permis d'améliorer globalement la production de binaires et réduire les coûts de compilation. Aujourd'hui les ordinateurs ont des composants très rapides. La lecture des instructions est plus rapide que l'exécution desdites instructions.\smallbreak

\subsection*{Prompt état de l'art des processeurs}

Le rapport de \citeauthor{constantTimePornin} \cite{constantTimePornin} décrit très bien les mécanismes employés par les processeurs pour accélérer l'exécution de programmes. Une modélisation d'un processus peut être présentée ainsi :
\begin{enumerate}
    \item Charge l'instruction pointé par le compteur de programme, incrémente ce dernier.
    \item Décode l'instruction.
    \item Exécute l'instruction.
\end{enumerate}

La dernière étape peut induire une modification de la mémoire, des chargements ou modifications de valeurs de registre ou résoudre des calculs. Tandis que chaque étape peut se résoudre selon un même rythme, cette troisième opération, à cause de sa variance, peut entraîner des ralentissements. Les constructeurs de matériels ont donc développé des techniques pour accélérer le temps total d'exécution et permettre ainsi des gains de performances. \smallbreak

Voici les principales techniques employées :

\begin{itemize}
    \item[\textbf{Pipeline}] Permet l'exécution parallèle de différentes instructions. Les instructions se résolvent toujours dans l'ordre abstrait, mais l’exécution de la suivante commence alors que la précédente (et éventuellement d’autres encore en cours) n’est pas terminée.
    \item[\textbf{Prédiction de branche}] Pré-chargement de branchement. Grâce à la méthode précédente, le processeur peut avancer dans son préchargement d'instructions en explorant les deux côtés d'un branchement mémoire puis défausser la branche inutile.
    \item[\textbf{Renommage de registres}] Modification des registres employés pour éviter les dépendances. Grâce au Pipeline, il peut y avoir des ralentissements dans la prédiction à cause de valeurs dépendantes entre les instructions. Cette méthode emploie un autre registre temporairement pour simuler le comportement attendu.
    \item[\textbf{Micro-opérations}] Utilisation d'un encodage interne pour découper les instructions binaires en unités plus petites. Cette opération invisible pour le développeur permet d'optimiser le binaire (\eg combiner une comparaison et un branchement en une seule opération).
    \item[\textbf{Exécution désordonné}\footnote{Plus connus sous le terme technique de référence anglais : Out-of-order Execution}] Certaines opérations demande malgré tout plus de temps d'exécution, grâce à sa capacité à lire et traduire en avance les opérations à réaliser, le processeur peut réordonner la file d'exécution.
\end{itemize}\medbreak

Ces résultats impliquent une rupture de la politique temps constant et sont invisibles aux yeux du développeur. Une connaissance approfondie sur le fonctionnement des processeurs est nécessaire pour avoir connaissance de ces éléments et des possiblités de les contrecarrer. Intel et ARM ont mis à disposition des variables à spécifier pour permettre la désactivation de telles optimisations pour certaines opérations (multiplication et division d'entier). Ces solutions partielles ne permettent pas d'avoir des garanties sur l'ensemble du binaire que l'on exécute.

\subsection*{Travaux de recherche au niveau du processeur}

Les architectures portées par Intel et ARM sont propriétaires et donc nous offrent peu de possibilités d'évolution. RISC-V de l'autre côté, grâce à son enregistrement dans l'espace commun peut accueillir des travaux d'universitaires. Cela a permis à des équipes Inria de développer des mécanismes au niveau du processeur qui permettent de garantir une exécution sans fuite par canal auxiliaire. Les travaux de \citeauthor{twartingCT} \cite{twartingCT} proposent l'ajout d'instructions \textit{lock} et \textit{unlock} dans l'ISA de l'architecture pour encapsuler des opérations qui doivent être réalisées avec des accès au cache mémoire. Cette protection vient avec l'ajout d'une nouvelle structure microarchitecturale à ajouter sur une carte mère.\smallbreak

Plus récemment, \citeauthor{cryptoeprint:2025/1331} \cite{cryptoeprint:2025/1331} proposent un circuit logique qui réalise une opération de masquage et permet de conserver les propriétés temps constant d'un binaires.

